# -*- coding: utf-8 -*-
"""harry-potter-rag-llamaindex-chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NeJSZ8w1_uOVroKSE91kwkMMLD-Eqx3h
"""

pip install llama-index-core llama-index-llms-google-genai llama-index-embeddings-google-genai

!pip install nest-asyncio

import os
# --- FIX ---
import nest_asyncio
nest_asyncio.apply()
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.google_genai import GoogleGenAI
from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
from llama_index.core.memory import ChatMemoryBuffer

# --- Step 2: Set API Key and Configuration ---
# IMPORTANT: Replace the placeholder with your actual Gemini API Key
GEMINI_API_KEY = "AIzaSyCudyALGdnDnfhHTDpNsClJi0ia4yls4qA"

if not GEMINI_API_KEY or GEMINI_API_KEY == "YOUR_API_KEY_HERE":
    raise ValueError("Please set your GEMINI_API_KEY to proceed.")

# Configure LLM and Embedding models
print("Configuring LlamaIndex settings with Gemini models...")
Settings.llm = GoogleGenAI(
    model_name="gemini-2.5-flash",
    api_key=GEMINI_API_KEY
)

Settings.embed_model = GoogleGenAIEmbedding(
    model_name="models/text-embedding-004",
    api_key=GEMINI_API_KEY
)

# Set chunk size for RAG processing
Settings.chunk_size = 512
Settings.chunk_overlap = 64

# --- Step 3: Load Documents ---
print("Loading Harry Potter book...")
# Specify the exact path to your uploaded PDF file
DOC_PATH = "/content/harrypotter.pdf"
if not os.path.exists(DOC_PATH):
    raise FileNotFoundError(f"Document not found at: {DOC_PATH}. Ensure the file is named correctly.")

documents = SimpleDirectoryReader(input_files=[DOC_PATH]).load_data()
print(f"Loaded {len(documents)} document chunks from {DOC_PATH}")

# --- Step 4: Create Vector Index (RAG Knowledge Base) ---
print("Creating Vector Index...")
index = VectorStoreIndex.from_documents(documents)
print("Index creation complete. Chatbot is ready.")

# --- Step 5: Create Chat Engine (Chatbot) ---
# Use a Chat Engine to maintain conversation history (memory)
chat_engine = index.as_chat_engine(
    chat_mode="condense_question",
    memory=memory,
    context_system_prompt=(
        "You are a helpful assistant for the Harry Potter universe. "
        "Your responses must be based ONLY on the provided context from the Harry Potter books. "
        "Be conversational, informative, and detailed where possible."
    ),
    verbose=True
)

question2 = "Who is Harry?"
print(f"ðŸ‘¤ USER: {question2}")
response2 = chat_engine.chat(question2)
print(f"ðŸ¤– BOT: {response2.response}\n")

# --- Step 6: Start Chat Loop ---
def run_chatbot():
    print("\n" + "="*40)
    print("Harry Potter RAG Chatbot Initialized ")
    print("Ask me anything about the attached books (type 'quit' to exit).")
    print("="*40)

    # Clear chat history from previous runs (if any)
    chat_engine.reset()

    while True:
        try:
            query = input("You: ")
            if query.lower() in ['quit', 'exit']:
                print("Goodbye!")
                break

            print("Chatbot: Thinking...")
            response = chat_engine.chat(query)
            print(f"Chatbot: {response.response}")

        except Exception as e:
            print(f"An error occurred: {e}")
            break

# Execute the chatbot function
run_chatbot()

